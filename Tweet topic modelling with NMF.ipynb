{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a48399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_excel('/Users/raphael/Desktop/Twitter Data/MergedFile_s_t_cleaned_copy.xlsx')\n",
    "df = df.dropna(subset = 'new_text_farm')\n",
    "\n",
    "# transform text to vector form\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_df=0.9, min_df=5)\n",
    "# apply transformation\n",
    "tf = vectorizer.fit_transform(df['new_text_farm'])\n",
    "tf = tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the NMF model to short-text (tweet) topic modelling\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "df = df.dropna(subset = 'new_text_farm')\n",
    "# Create a TfidfVectorizer object to convert the text data to a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_df=0.9, min_df=5)\n",
    "\n",
    "# Fit and transform the text data into TF-IDF features\n",
    "tfidf = vectorizer.fit_transform(df['new_text_farm'])\n",
    "\n",
    "# Create an NMF object with n topics\n",
    "nmf = NMF(n_components=23)\n",
    "\n",
    "# Fit the NMF model to the TF-IDF features\n",
    "nmf.fit(tfidf)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Show the top 12 most likely words in a topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i]) for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i]) for i in topic.argsort()[:-no_top_words - 1:-1]]                       \n",
    "    return pd.DataFrame(topic_dict)\n",
    "\n",
    "no_top_words = 12\n",
    "topic_df = display_topics(nmf, feature_names, no_top_words)\n",
    "display(topic_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c85643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the representative sentence of each topic\n",
    "\n",
    "# Get the topic assignments for each document\n",
    "doc_topic_probs = nmf.transform(tfidf)\n",
    "\n",
    "# Get the topic-word probabilities\n",
    "topic_word_probs = nmf.components_\n",
    "\n",
    "#Create an empty list for sentences\n",
    "topic_sentences = []\n",
    "representative = []\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "topic_index = np.argmax(doc_topic_probs, axis=1)\n",
    "\n",
    "# Loop through each topic\n",
    "for topic_idx in range(nmf.n_components):\n",
    "    # Loop through each document to categorize the sentences\n",
    "    for i in range(len(doc_topic_probs)):\n",
    "        if topic_index[i] == topic_idx:\n",
    "            sentences = sent_tokenize(df['new_text_farm'].iloc[i])  # Tokenize document into sentences\n",
    "            topic_sentences.extend(sentences)  # Add sentences to topic_sentences list\n",
    "\n",
    "    # Get the sentence with the highest topic probability\n",
    "    if topic_sentences:\n",
    "        topic_sentence_probs = nmf.transform(vectorizer.transform(topic_sentences))\n",
    "        most_representative_sentence_idx = np.argmax(topic_sentence_probs[:, topic_idx])\n",
    "        most_representative_sentence = topic_sentences[most_representative_sentence_idx]\n",
    "        representative.append(f\"Topic {topic_idx}: {most_representative_sentence}\")\n",
    "    else:\n",
    "        representative.append(f\"Topic {topic_idx}: No sentence available\")\n",
    "        \n",
    "SentenceList = pd.DataFrame(representative)\n",
    "print(SentenceList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document â€” Topic Matrix\n",
    "\n",
    "output = nmf.transform(tfidf) \n",
    "# column names\n",
    "topicnames = ['Topic' + str(i) for i in range(nmf.n_components)]\n",
    "# index names\n",
    "docnames = ['Post' + str(i) for i in range(len(df['new_text_farm']))]\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(output, columns=topicnames, index=docnames)\n",
    "# Normalize the values in the DataFrame\n",
    "for i in range(len(df_document_topic)):\n",
    "    row_sum = df_document_topic.iloc[i].sum()\n",
    "    df_document_topic.iloc[i] = df_document_topic.iloc[i] / row_sum  #normalize the matrix\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.where(df_document_topic.max(axis=1)>=0.35, np.argmax(df_document_topic.values, axis=1), np.nan)\n",
    "\n",
    "dominant_topic1 = df_document_topic.apply(lambda x: x.argsort()[::-1][1], axis=1)\n",
    "\n",
    "\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "df_document_topic['dominant_topic1'] = dominant_topic1\n",
    "\n",
    "print(df_document_topic.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
