{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ef67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the topic modelling result - top 12 most likely words of topics\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)\n",
    "\n",
    "no_top_words = 12\n",
    "display_topics(model, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b023fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the topic modelling result - visualize the content of topics\n",
    "\n",
    "import pyLDAvis.sklearn\n",
    "# Initialize\n",
    "pyLDAvis.enable_notebook()\n",
    "fig = pyLDAvis.sklearn.prepare(model, vectorizer.fit_transform(df['CleanText']), vectorizer,  mds='tsne', R=40, sort_topics=False)\n",
    "pyLDAvis.display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8782bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the topic modelling result - the most representative sentence of each topic\n",
    "\n",
    "# Get the topic assignments for each document\n",
    "doc_topic_probs = model.transform(tf)\n",
    "# Get the topic-word probabilities\n",
    "topic_word_probs = model.components_\n",
    "#Create an empty list for sentences\n",
    "topic_sentences = []\n",
    "representative = []\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "topic_index = np.argmax(doc_topic_probs, axis=1)\n",
    "# Loop through each topic\n",
    "for topic_idx in range(model.n_components):\n",
    "    # Loop through each document to categorize the sentences\n",
    "    for i in range(len(doc_topic_probs)):\n",
    "        if topic_index[i] == topic_idx:\n",
    "            sentences = sent_tokenize(df['CleanText'][i])  # Tokenize document into sentences\n",
    "            topic_sentences.extend(sentences)  # Add sentences to topic_sentences list\n",
    "\n",
    "    # Get the sentence with the highest topic probability\n",
    "    if topic_sentences:\n",
    "        topic_sentence_probs = model.transform(vectorizer.transform(topic_sentences))\n",
    "        most_representative_sentence_idx = np.argmax(topic_sentence_probs[:, topic_idx])\n",
    "        most_representative_sentence = topic_sentences[most_representative_sentence_idx]\n",
    "        representative.append(f\"Topic {topic_idx}: {most_representative_sentence}\")\n",
    "    else:\n",
    "        representative.append(f\"Topic {topic_idx}: No sentence available\")\n",
    "        \n",
    "\n",
    "SentenceList = pd.DataFrame(representative)\n",
    "print(SentenceList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
